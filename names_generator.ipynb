{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmigas/Projects/blob/main/names_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KxEDfxFHZgO"
      },
      "source": [
        "The objective of this project is to check how adding randomnes to a text-generating model adds quality to the end product.\n",
        "\n",
        "The basis of the model is a simple LSTM network. Its task is to learn English and Polish names and later generate names using different schemes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz3ecmo7J9_6"
      },
      "source": [
        "Let's start with the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ggQHdFzGlTe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import os\n",
        "os. environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU8gYWLSaZKK"
      },
      "source": [
        "Let's start with loading data - Polish and English names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUh90YyJSfST"
      },
      "outputs": [],
      "source": [
        "pnames = pd.read_csv('/content/drive/MyDrive/Python_data/P_names.csv')\n",
        "enames = pd.read_csv('/content/drive/MyDrive/Python_data/E_names.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unVB3MAEbRdk",
        "outputId": "868380f1-3d24-441f-d0c6-2d9c8e2b1c3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Polish names in a database: 1711\n",
            "Number of English names in a database: 18238\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of Polish names in a database: {len(pnames)}')\n",
        "print(f'Number of English names in a database: {len(enames)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxaSUfZLboZA"
      },
      "source": [
        "We have over 10 times as many English names as Polish names. We'll see how it affects the quality of what our model can produce.\n",
        "\n",
        "A fan fact for English readers. In the USA if you have a kid, you can use any word that you wish as his/her name, your fantasty and your warm feelings towards your kid's future in a school is your limit.\n",
        "\n",
        "In Poland (and probably in many other countries) you can only choose from an officially approved list of authorized names. Probably that is a reason why a list of Polish names is over 10 times shorter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Pi96_pP-b63u",
        "outputId": "8dea7f82-6312-48c1-8fe6-b0cc55b0b6bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8af9bbb4-a2eb-40a2-99c0-3ed9946dd339\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>michael</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>christopher</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jessica</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>matthew</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ashley</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8af9bbb4-a2eb-40a2-99c0-3ed9946dd339')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8af9bbb4-a2eb-40a2-99c0-3ed9946dd339 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8af9bbb4-a2eb-40a2-99c0-3ed9946dd339');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          name  len\n",
              "0      michael    7\n",
              "1  christopher   11\n",
              "2      jessica    7\n",
              "3      matthew    7\n",
              "4       ashley    6"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enames.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Ni-N3Hb-kc"
      },
      "source": [
        "I preprocessed both files beforehead to save your time here. In both files we have no duplicates, all names lower cased and a 'len' column added with a char count for each name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq80U7vVMYoR"
      },
      "source": [
        "Our model is naturally a char-level generating model. It's core architecture and data stracture can be easily copied from any deep learning manual.\n",
        "\n",
        "What is interesting here is how we apply randomness at two levels:\n",
        "1. at the learning process by replacing some \"next char\" labels with random chars\n",
        "2. at the generating process by applying what I call a \"simplified Beam\" in place of a greedy search routine\n",
        "\n",
        "Will these two result in better, richer, more interesting and natural names? Let's see!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-pm86mLyZmU"
      },
      "source": [
        "Let's start with English names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qwtKPAryx73"
      },
      "outputs": [],
      "source": [
        "names = enames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S26PCbwxy1Yz"
      },
      "source": [
        "We need a full list of chars used in English names and length of the longest name.\n",
        "As a standard procedure in such models, we also build char_to_int and int_to_char dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hdFEB81y9th",
        "outputId": "db00f1cf-e312-4b7b-ff5c-aff0cb08174f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The longest name is 15 chars long.\n",
            "Chars list: [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Number of unique chars, including white space: 27\n"
          ]
        }
      ],
      "source": [
        "max_len = names['len'].max()\n",
        "\n",
        "nlist = list(names['name'])\n",
        "text = ' '.join(nlist)\n",
        "chars = sorted(list(set(text)))\n",
        "print(f'The longest name is {max_len} chars long.')\n",
        "print(f\"Chars list: {chars}\")\n",
        "print(f\"Number of unique chars, including white space: {len(chars)}\")\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URYPBZlH1LBz"
      },
      "source": [
        "Now we begin to build our X and Y datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR2II3Y81THb",
        "outputId": "312065d0-bd91-404e-e4d5-a4f581ae9749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X data\n",
            "[[13], [13, 9], [13, 9, 3], [13, 9, 3, 8], [13, 9, 3, 8, 1], [13, 9, 3, 8, 1, 5], [13, 9, 3, 8, 1, 5, 12], [3], [3, 8], [3, 8, 18], [3, 8, 18, 9], [3, 8, 18, 9, 19], [3, 8, 18, 9, 19, 20], [3, 8, 18, 9, 19, 20, 15], [3, 8, 18, 9, 19, 20, 15, 16], [3, 8, 18, 9, 19, 20, 15, 16, 8], [3, 8, 18, 9, 19, 20, 15, 16, 8, 5], [3, 8, 18, 9, 19, 20, 15, 16, 8, 5, 18], [10], [10, 5]]\n",
            "Y data\n",
            "[9, 3, 8, 1, 5, 12, 0, 8, 18, 9, 19, 20, 15, 16, 8, 5, 18, 0, 5, 19]\n"
          ]
        }
      ],
      "source": [
        "dataX = [] # x data\n",
        "dataY_original = [] # y data (labels) - simply the next char\n",
        "\n",
        "for name in names['name']:\n",
        "    for i in range(len(name)):\n",
        "        if i < len(name) - 1:\n",
        "            dataX.append([char_to_int[char] for char in name[0:i+1]])\n",
        "            dataY_original.append(char_to_int[name[i+1]])\n",
        "        else:\n",
        "            dataX.append([char_to_int[char] for char in name[0:i+1]])\n",
        "            dataY_original.append(0)\n",
        "\n",
        "print('X data')\n",
        "print(dataX[:20])\n",
        "print('Y data')\n",
        "print(dataY_original[:20])\n",
        "originalY = dataY_original[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh4jfYCH1nHP"
      },
      "source": [
        "A label (Y data) is simply the next char in a name. That is how all char-based models work at the train phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpqJxSB114Rb"
      },
      "source": [
        "Now we begin adding randomness to the model.\n",
        "At first, we declare a RAND_Y value, which tells, every which char we replace by a random one. If RAND_Y equals zero, there's no randomness and all labels remain unchained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku1RLcxV2Yg6"
      },
      "outputs": [],
      "source": [
        "def get_Y(dataY, RAND_Y = 0):\n",
        "  if RAND_Y > 0:\n",
        "      for i, y in enumerate(dataY):\n",
        "          if y != 0:\n",
        "              if np.random.randint(0, RAND_Y) == 0: # on average, we will replace every RAND_Yth char with a randomly selected char\n",
        "                  dataY[i] = np.random.randint(1, len(chars))\n",
        "  return dataY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crXljCTb6Xd1"
      },
      "source": [
        "Let's see how the first 10 labels look like after various RAND_Y values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7tIC6ks99n8",
        "outputId": "792e52df-3c3e-4d44-fb2c-580de6942b93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No randomness\n",
            "Original Y: [9, 3, 8, 1, 5, 12, 0, 8, 18, 9, 19, 20, 15, 16, 8, 5, 18, 0, 5, 19]\n",
            "Modified Y: [9, 3, 8, 1, 5, 12, 0, 8, 18, 9, 19, 20, 15, 16, 8, 5, 18, 0, 5, 19]\n"
          ]
        }
      ],
      "source": [
        "print(\"No randomness\")\n",
        "dataY = get_Y(dataY_original, RAND_Y = 0)\n",
        "RAND_Y = 0\n",
        "print(f\"Original Y: {originalY}\")\n",
        "print(f\"Modified Y: {dataY[:20]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCYeQsjl9d_Y",
        "outputId": "6c8ae4b9-4688-4f60-e8da-ba162c0bc6c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Every 7th modified\n",
            "Original Y: [9, 3, 8, 1, 5, 12, 0, 8, 18, 9, 19, 20, 15, 16, 8, 5, 18, 0, 5, 19]\n",
            "Modified Y: [7, 3, 8, 1, 3, 12, 0, 8, 18, 12, 19, 20, 15, 16, 8, 5, 18, 0, 5, 19]\n"
          ]
        }
      ],
      "source": [
        "RAND_Y = 7\n",
        "print(f\"Every {RAND_Y}th modified\")\n",
        "dataY = get_Y(dataY_original, RAND_Y)\n",
        "\n",
        "print(f\"Original Y: {originalY}\")\n",
        "print(f\"Modified Y: {dataY[:20]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYZosa53DqCN"
      },
      "source": [
        "Now we need to prepare data so that they fit an LSTM network.\n",
        "All steps are basic and self-explanatory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDchOWqqD05W",
        "outputId": "bf43145b-62c7-4ab8-b5a5-2453cd41a26f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (115316, 15, 1)\n",
            "Y shape: (115316, 27)\n"
          ]
        }
      ],
      "source": [
        "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
        "num_of_chars = max(dataY)\n",
        "X = X/num_of_chars\n",
        "Y = np_utils.to_categorical(dataY)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"Y shape: {Y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-oq94JIH2uf"
      },
      "source": [
        "Let's build our model. Two layers of Bidirectional LSTM with a serious dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIfr22rqHxCm"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "  model = Sequential()\n",
        "  model.add(Bidirectional(LSTM(16, return_sequences=True, input_shape=(X.shape[1], 1))))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Bidirectional(LSTM(16)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyaQK8BWXJz5"
      },
      "source": [
        "We've already seen how randomness is added at a training data level. Now let's see how a \"simplified Beam\" is added at a text generating level.\n",
        "\n",
        "A simple choice is a greedy serach. After a predict method produces a vector of \"probabilities\" for each char, we choose the one with the highest value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0_bducTZnFw"
      },
      "outputs": [],
      "source": [
        "c = model.predict(wordX.reshape(1, max_len, 1), verbose = 0) # a vector\n",
        "c_max = int(c.argmax(axis=1)) # amvalue with a max probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvBlFCnCZs5v"
      },
      "source": [
        "An alternative is to choose between a couple of values with the highest score from the probability method. We can randomly choose from a list of all chars using a probability vector as a p parameter in an np.random.choice method.\n",
        "\n",
        "However, if we have a more or less flat vector of probabilities, like let's say 'a' = 0.05, 'b' = 0.07, 'c' = 0.08 etc. we would end with a chaotic noise rather than any interesting output sounding like a real name.\n",
        "\n",
        "To avoid this, we can penalize lower probability values and multiply higher values with a simple trick. We take each vector value and raise to power of MULTIPLIER level. Later, we normalize all values so that all probabilities sum up to 1 and can be used as a p parameter in an np.random.choice method.\n",
        "\n",
        "I usually used MULTIPLIER = 3 value. Let's see how it works.\n",
        "Let's say we have ten chars in our alphabet. We will generate a vector of probabilities for each char and normalize so that they sum up to 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YJZL-JcpvNS",
        "outputId": "3c6a4f8b-a641-43c0-93ec-41e891d1063f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.2  0.19 0.17 0.14 0.12 0.09 0.03 0.02 0.02 0.01]\n",
            "0.56\n"
          ]
        }
      ],
      "source": [
        "v = np.random.random(10)\n",
        "v = v / sum(v)\n",
        "v = np.sort(v)[::-1]\n",
        "v = np.round(v, 2)\n",
        "print(v)\n",
        "print(np.sum(v[:3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVLVicmEp2xl"
      },
      "source": [
        "Let's assume, that we treat top 2-3 chars as \"good\", which means that choosing out of them we expect to produce a sensible name that \"sounds good\". We should somehow minimze the probability of choosing out of the remaining 7-8 chars, but still keep the order of top 2-3 chars in terms of their probability. On the other hand, we do not want to simply cut off the long tail, rather diminish its probability.\n",
        "\n",
        "Let's see how applying a \"simplified Beam\" with MULTIPLIER = 3 changes our vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TFWOet3qopC"
      },
      "outputs": [],
      "source": [
        "MULTIPLIER = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTZaXOSeqlLl",
        "outputId": "f52ec593-a515-444a-ffae-9e09d6a2a722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.32 0.27 0.2  0.11 0.07 0.03 0.   0.   0.   0.  ]\n",
            "0.79\n"
          ]
        }
      ],
      "source": [
        "c = [x ** MULTIPLIER for x in v] #premiujemy największe prawdopodobieństwa\n",
        "c = [x / sum(c) for x in c]\n",
        "c = np.round(c, 2)\n",
        "print(c)\n",
        "print(np.sum(c[:3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44cJwJ5cZrfs"
      },
      "source": [
        "As we can see, two top values (0.3 and 0.44) are strengthened and now sum up to 0.93, the other values were made significant.\n",
        "On the other hand, it's still much more random than greedy search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnubSvWr2J33"
      },
      "source": [
        "Now let's see how it looks like with different MULTIPLIER values (from 2 to 6) on a 10 char vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN5wUHZX2S9S",
        "outputId": "ab14f237-547a-46cb-ab01-ef02ff0acab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vector sorted:\n",
            "[0.16 0.16 0.12 0.11 0.08 0.08 0.08 0.08 0.07 0.05]\n",
            "Sum of the first three values: 0.44\n",
            "\n",
            "Vector with multiplier 2:\n",
            "[0.23 0.23 0.13 0.11 0.06 0.06 0.06 0.06 0.04 0.02]\n",
            "Sum of the first three values: 0.59\n",
            "\n",
            "Vector with multiplier 3:\n",
            "[0.3  0.3  0.13 0.1  0.04 0.04 0.04 0.04 0.02 0.01]\n",
            "Sum of the first three values: 0.73\n",
            "\n",
            "Vector with multiplier 4:\n",
            "[0.35 0.35 0.11 0.08 0.02 0.02 0.02 0.02 0.01 0.  ]\n",
            "Sum of the first three values: 0.81\n",
            "\n",
            "Vector with multiplier 5:\n",
            "[0.39 0.39 0.09 0.06 0.01 0.01 0.01 0.01 0.01 0.  ]\n",
            "Sum of the first three values: 0.87\n",
            "\n",
            "Vector with multiplier 6:\n",
            "[0.42 0.42 0.08 0.04 0.01 0.01 0.01 0.01 0.   0.  ]\n",
            "Sum of the first three values: 0.92\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def beam(vec_len, multiplier):\n",
        "    v = np.random.random(vec_len)\n",
        "    v = v / sum(v)\n",
        "    v = np.sort(v)[::-1]\n",
        "    v = np.round(v, 2)\n",
        "    print('Original vector sorted:')\n",
        "    print(v)\n",
        "    print(f\"Sum of the first three values: {np.sum(v[:3]):.2f}\\n\")\n",
        "\n",
        "    for m in multiplier:\n",
        "        c = [x ** m for x in v] #premiujemy największe prawdopodobieństwa\n",
        "        c = [x / sum(c) for x in c]\n",
        "        c = np.round(c, 2)\n",
        "        print(f\"Vector with multiplier {m}:\")\n",
        "        print(c)\n",
        "        print(f\"Sum of the first three values: {np.sum(c[:3]):.2f}\\n\")\n",
        "\n",
        "\n",
        "beam(vec_len = 10, multiplier = [2, 3, 4, 5, 6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tyHnUlp2585"
      },
      "source": [
        "I will try 3 and 6 values in a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCq6xL-UO5tL"
      },
      "source": [
        "So, we are now ready with the model.\n",
        "We will generate 10 series of names, 10 in each serie.\n",
        "The model is compiled once at the beginning and it is fit with the data in each series. We will observe how the quality of the generated names gets augmented (or does not?) with each consecitive fit for 10 epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ5qCz03lyyt"
      },
      "source": [
        "Let's start with a greedy search and no randomnes in training data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcchQNZPLYDy",
        "outputId": "efaf3480-21d5-4860-e50a-8760a71628b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandY: 0\n",
            "Greedy search: False\n",
            "Series  1 names: ['wanesha ', 'tanesha ', 'yanesha ', 'narin ', 'garisha ', 'garisha ', 'parin ', 'quan ', 'sharin ', 'tanesha ']\n",
            "Series  2 names: ['kamesha ', 'farrin ', 'parin ', 'marin ', 'zanesha ', 'delisha ', 'tanesha ', 'harin ', 'farrin ', 'shanna ']\n",
            "Series  3 names: ['zachell ', 'harrin ', 'larisha ', 'yachell ', 'larisha ', 'ranina ', 'antell ', 'tarin ', 'nlena ', 'harrin ']\n",
            "Series  4 names: ['wanesha ', 'istan ', 'marisha ', 'urenta ', 'nrenta ', 'urenta ', 'qunell ', 'yanesha ', 'ranesha ', 'elista ']\n",
            "Series  5 names: ['elista ', 'jamisha ', 'latan ', 'farris ', 'ista ', 'elista ', 'urenta ', 'nristin ', 'brittin ', 'urenta ']\n",
            "Series  6 names: ['ranesha ', 'urena ', 'jamina ', 'harrie ', 'yanesha ', 'walina ', 'lataria ', 'ishanna ', 'antnn ', 'charlina ']\n",
            "Series  7 names: ['isanna ', 'urentin ', 'quanna ', 'gerrica ', 'elisha ', 'gerrica ', 'darricia ', 'latara ', 'urentin ', 'walena ']\n",
            "Series  8 names: ['yanesha ', 'willian ', 'brittin ', 'harrick ', 'gerrica ', 'tarina ', 'villian ', 'charlina ', 'villian ', 'marisha ']\n",
            "Series  9 names: ['vinisha ', 'yanesha ', 'marisha ', 'yanesha ', 'harrin ', 'tanesha ', 'urenta ', 'xanesha ', 'tanesha ', 'gerrica ']\n",
            "Series 10 names: ['antnn ', 'elisha ', 'randi ', 'antnn ', 'walina ', 'shanell ', 'natasha ', 'walina ', 'isabell ', 'quanna ']\n",
            "ALL NAMES GENERATED after 3463.29 sekund\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "names_full = []\n",
        "\n",
        "not_starting = list(np.arange(26, 33)) # only applies to Polish names\n",
        "not_starting = [] # empty for English names\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "print(f\"RandY: {RAND_Y}\")\n",
        "print(f\"Greedy search: {RANDOM_CHARS}\")\n",
        "tic = time()\n",
        "\n",
        "for i in range(SERIES):\n",
        "    tic = time()\n",
        "    model.fit(X, Y, epochs=EPOCHS, verbose=0, batch_size=64)\n",
        "    names_ = []\n",
        "    if i % 1 == 0:\n",
        "        for k in range(10):\n",
        "            n = np.random.randint(1, num_of_chars + 1) #we genearate the first char of the name\n",
        "            while n in not_starting: #used only for Polish names\n",
        "                n = np.random.randint(1, num_of_chars + 1)\n",
        "            n = n/num_of_chars #scaling for an LSTM model\n",
        "\n",
        "            word = [[]]\n",
        "            word[0].append(n)\n",
        "            for j in range(max_len):\n",
        "                wordX = pad_sequences(word, maxlen=max_len, dtype='float32')\n",
        "                c = model.predict(wordX.reshape(1, max_len, 1), verbose = 0)\n",
        "                if RANDOM_CHARS == True:\n",
        "                    c = list(c.flatten())\n",
        "                    c = [x ** MULTIPLIER for x in c]\n",
        "                    c = [x / sum(c) for x in c]\n",
        "                    c_max = np.random.choice(chars, p = c)\n",
        "                    c_max = char_to_int[c_max]\n",
        "                else:\n",
        "                    c_max = int(c.argmax(axis=1))\n",
        "                word[0].append(c_max/num_of_chars)\n",
        "                if c_max == 0:\n",
        "                    break\n",
        "            imie = ''.join([int_to_char[int(char*num_of_chars)] for char in word[0]])\n",
        "\n",
        "            names_.append(imie)\n",
        "\n",
        "        print(f\"Series {i+1:>2} names: {names_} after {(time() - tic)/60:.2f} minutes.\")\n",
        "        names_full.append(names_)\n",
        "\n",
        "        df = pd.DataFrame(names_full)\n",
        "\n",
        "print(f'ALL NAMES GENERATED after {time() - tic:.2f} seconds')\n",
        "df.to_csv('/content/drive/MyDrive/Python_data/English_greedy_norand.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRsg99QQk9bU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import random as python_random\n",
        "from time import time\n",
        "\n",
        "def reset_seeds():\n",
        "   np.random.seed(123)\n",
        "   python_random.seed(123)\n",
        "   tf.random.set_seed(1234)\n",
        "\n",
        "def get_data(rand_y):\n",
        "    names = pd.read_csv('/content/drive/MyDrive/Python_data/E_names.csv')\n",
        "    max_len = names['len'].max()\n",
        "\n",
        "    nlist = list(names['name'])\n",
        "    text = ' '.join(nlist)\n",
        "    chars = sorted(list(set(text)))\n",
        "\n",
        "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "    dataX = [] # x data\n",
        "    dataY = [] # y data (labels) - simply the next char\n",
        "\n",
        "    for name in names['name']:\n",
        "        for i in range(len(name)):\n",
        "            if i < len(name) - 1:\n",
        "                dataX.append([char_to_int[char] for char in name[0:i+1]])\n",
        "                dataY.append(char_to_int[name[i+1]])\n",
        "            else:\n",
        "                dataX.append([char_to_int[char] for char in name[0:i+1]])\n",
        "                dataY.append(0)\n",
        "\n",
        "    if rand_y > 0:\n",
        "        for i, y in enumerate(dataY):\n",
        "            if y != 0:\n",
        "                if np.random.randint(0, rand_y) == 0: # on average, we will replace every RAND_Yth char with a randomly selected char\n",
        "                    # print('losowo')\n",
        "                    dataY[i] = np.random.randint(1, len(chars))\n",
        "\n",
        "\n",
        "    X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
        "    num_of_chars = max(dataY)\n",
        "    X = X/num_of_chars\n",
        "    Y = np_utils.to_categorical(dataY)\n",
        "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "    return X, Y, max_len, char_to_int, int_to_char, num_of_chars, chars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV8kbbzjn0W_"
      },
      "source": [
        "For speed, we will use a slightly smaller network with 16 neurons in each of two LSTM layers.\n",
        "For repeatability, we will reset seed each time at the model restart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxYxcohzljYw"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    reset_seeds()\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(16, return_sequences=True, input_shape=(X.shape[1], 1))))\n",
        "    # model.add(Bidirectional(LSTM(16, input_shape=(X.shape[1], 1))))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Bidirectional(LSTM(16)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTClKuuntyka"
      },
      "outputs": [],
      "source": [
        "not_starting = list(np.arange(26, 33)) # only applies to Polish names\n",
        "not_starting = [] # only applies to Polish names\n",
        "\n",
        "RAND_Y = [0, 3, 5]\n",
        "MULTIPLIER_true = [3, 6]\n",
        "MULTIPLIER_false = [1]\n",
        "BEAM = [False, True]\n",
        "EPOCHS = 5\n",
        "SERIES = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFzUazRdt55Z",
        "outputId": "d6fbdc4f-21c1-4c4f-e97e-da89903d6053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating: English__beam:False__multiplier:1__rand_y:0\n",
            "Series  1 names: ['narana ', 'carana ', 'carana ', 'garana ', 'rarana ', 'tarana ', 'karana ', 'zarana ', 'warana ', 'barana '] after 1.54 minutes.\n",
            "Series  2 names: ['arrian ', 'raria ', 'paria ', 'jaria ', 'arrian ', 'naria ', 'arrian ', 'paria ', 'zaria ', 'taria '] after 1.32 minutes.\n",
            "Series  3 names: ['narisa ', 'elisa ', 'anisan ', 'qarisa ', 'elisa ', 'rarisa ', 'xarisa ', 'darisa ', 'chana ', 'uarisa '] after 1.27 minutes.\n",
            "Series  4 names: ['harin ', 'chann ', 'uarin ', 'parin ', 'yarin ', 'qarin ', 'harin ', 'jarin ', 'darin ', 'uarin '] after 1.27 minutes.\n",
            "Series  5 names: ['garisa ', 'brisin ', 'varisa ', 'warisa ', 'chann ', 'brisin ', 'marisa ', 'iarisa ', 'darisa ', 'karisa '] after 1.28 minutes.\n",
            "Series  6 names: ['frisa ', 'annnn ', 'larisa ', 'chann ', 'karisa ', 'warisa ', 'narisa ', 'sarisa ', 'elisis ', 'parisa '] after 1.29 minutes.\n",
            "Series  7 names: ['larisha ', 'marisha ', 'uarisha ', 'garisha ', 'narisha ', 'tarisha ', 'qarisha ', 'garisha ', 'nhana ', 'uarisha '] after 1.34 minutes.\n",
            "Series  8 names: ['harisha ', 'larisha ', 'warisha ', 'harisha ', 'brista ', 'larisha ', 'farisha ', 'xarisha ', 'zarisha ', 'shana '] after 1.31 minutes.\n",
            "Series  9 names: ['uarisha ', 'yarisha ', 'rarisha ', 'marisha ', 'shana ', 'rarisha ', 'brista ', 'tarisha ', 'marisha ', 'varisha '] after 1.30 minutes.\n",
            "Series 10 names: ['jarisha ', 'qarisha ', 'yarisha ', 'rarisha ', 'warisha ', 'darisha ', 'darisha ', 'larisha ', 'varisha ', 'zarisha '] after 1.33 minutes.\n",
            "\n",
            "Generating: English__beam:False__multiplier:1__rand_y:3\n",
            "Series  1 names: ['nara ', 'carana ', 'carana ', 'garana ', 'rara ', 'tara ', 'kara ', 'zara ', 'wara ', 'barana '] after 1.42 minutes.\n",
            "Series  2 names: ['arara ', 'raran ', 'paran ', 'jara ', 'arara ', 'naran ', 'arara ', 'paran ', 'zanan ', 'taran '] after 1.31 minutes.\n",
            "Series  3 names: ['naria ', 'earia ', 'arana ', 'qaria ', 'earia ', 'raria ', 'xanan ', 'daria ', 'caria ', 'uanan '] after 1.30 minutes.\n",
            "Series  4 names: ['haria ', 'charia ', 'uania ', 'paria ', 'yania ', 'qaria ', 'haria ', 'jaria ', 'daria ', 'uania '] after 1.29 minutes.\n",
            "Series  5 names: ['garia ', 'bran ', 'vania ', 'wania ', 'charia ', 'bran ', 'maria ', 'iaria ', 'daria ', 'karia '] after 1.30 minutes.\n",
            "Series  6 names: ['faria ', 'anan ', 'laria ', 'chana ', 'karia ', 'wania ', 'naria ', 'shana ', 'earia ', 'paria '] after 1.31 minutes.\n",
            "Series  7 names: ['laria ', 'maria ', 'uania ', 'garia ', 'naria ', 'tania ', 'qaria ', 'garia ', 'naria ', 'uania '] after 1.30 minutes.\n",
            "Series  8 names: ['haria ', 'laria ', 'wania ', 'haria ', 'brian ', 'laria ', 'faria ', 'xania ', 'zania ', 'shana '] after 1.30 minutes.\n",
            "Series  9 names: ['uana ', 'yana ', 'rana ', 'maria ', 'shana ', 'rana ', 'brian ', 'tana ', 'maria ', 'vana '] after 1.32 minutes.\n",
            "Series 10 names: ['jarisha ', 'qarisha ', 'yanan ', 'raria ', 'wanan ', 'darisha ', 'darisha ', 'larisha ', 'vanan ', 'zanan '] after 1.31 minutes.\n",
            "\n",
            "Generating: English__beam:False__multiplier:1__rand_y:5\n",
            "Series  1 names: ['naran ', 'carana ', 'carana ', 'garana ', 'raran ', 'taran ', 'karan ', 'zaran ', 'waran ', 'barana '] after 1.46 minutes.\n",
            "Series  2 names: ['arana ', 'raran ', 'paran ', 'jaran ', 'arana ', 'naran ', 'arana ', 'paran ', 'zaran ', 'taran '] after 1.31 minutes.\n",
            "Series  3 names: ['nania ', 'earia ', 'anania ', 'qania ', 'earia ', 'rania ', 'xanana ', 'daria ', 'chana ', 'uanan '] after 1.31 minutes.\n",
            "Series  4 names: ['haria ', 'chana ', 'uania ', 'pania ', 'yania ', 'qania ', 'haria ', 'jaria ', 'daria ', 'uania '] after 1.32 minutes.\n",
            "Series  5 names: ['garia ', 'brian ', 'vania ', 'wania ', 'chana ', 'brian ', 'maria ', 'iaria ', 'daria ', 'karia '] after 1.30 minutes.\n",
            "Series  6 names: ['faria ', 'anian ', 'laria ', 'charia ', 'karia ', 'wania ', 'naria ', 'shana ', 'elan ', 'paria '] after 1.28 minutes.\n",
            "Series  7 names: ['laria ', 'maria ', 'uania ', 'garia ', 'naria ', 'tania ', 'qaria ', 'garia ', 'naria ', 'uania '] after 1.30 minutes.\n",
            "Series  8 names: ['harisha ', 'larisha ', 'wanisha ', 'harisha ', 'brisha ', 'larisha ', 'farisha ', 'xanisha ', 'zanan ', 'shana '] after 1.30 minutes.\n",
            "Series  9 names: ['uanan ', 'yanan ', 'ranan ', 'marisha ', 'shana ', 'ranan ', 'brisha ', 'tanan ', 'marisha ', 'vanan '] after 1.29 minutes.\n",
            "Series 10 names: ['jarisha ', 'qarisha ', 'yanan ', 'ranan ', 'wanan ', 'darisha ', 'darisha ', 'larisha ', 'vanan ', 'zanan '] after 1.31 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:3__rand_y:0\n",
            "Series  1 names: ['naneia ', 'arania ', 'earala ', 'yaria ', 'vanan ', 'alanana ', 'uaraa ', 'leana ', 'yarie ', 'yarla '] after 1.43 minutes.\n",
            "Series  2 names: ['ceris ', 'gelia ', 'karana ', 'panis ', 'erre ', 'mari ', 'mamia ', 'naran ', 'marne ', 'xhanea '] after 1.30 minutes.\n",
            "Series  3 names: ['barra ', 'narisa ', 'delert ', 'narra ', 'qhanin ', 'charnan ', 'rarina ', 'harin ', 'jarian ', 'rhania '] after 1.31 minutes.\n",
            "Series  4 names: ['varesia ', 'vanene ', 'genis ', 'pavin ', 'wasela ', 'varann ', 'tnnnn ', 'amera ', 'rarra ', 'narian '] after 1.31 minutes.\n",
            "Series  5 names: ['rarran ', 'alenna ', 'kalen ', 'harhie ', 'tene ', 'tarin ', 'ieann ', 'parinn ', 'farane ', 'wanna '] after 1.30 minutes.\n",
            "Series  6 names: ['harin ', 'uarha ', 'iasin ', 'ariene ', 'varise ', 'uarin ', 'zaresa ', 'elisis ', 'renin ', 'jaren '] after 1.33 minutes.\n",
            "Series  7 names: ['jarisha ', 'alisan ', 'narrisha ', 'alanin ', 'uarin ', 'garren ', 'varisa ', 'chan ', 'erenan ', 'daruan '] after 1.32 minutes.\n",
            "Series  8 names: ['varia ', 'aranda ', 'qanana ', 'ianna ', 'yamell ', 'varisa ', 'xarisha ', 'nerisha ', 'haman ', 'waran '] after 1.31 minutes.\n",
            "Series  9 names: ['latnn ', 'zarian ', 'xarisha ', 'larina ', 'pavala ', 'marisha ', 'qanisha ', 'ratsen ', 'verena ', 'zarisa '] after 1.30 minutes.\n",
            "Series 10 names: ['larri ', 'narin ', 'rarisha ', 'pnrisha ', 'marrin ', 'karina ', 'brisha ', 'larini ', 'rarrnn ', 'marrin '] after 1.34 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:6__rand_y:0\n",
            "Series  1 names: ['narea ', 'warana ', 'naria ', 'harana ', 'garana ', 'ananana ', 'uara ', 'uaran ', 'sania ', 'jarie '] after 1.41 minutes.\n",
            "Series  2 names: ['haria ', 'xaran ', 'narre ', 'naria ', 'brria ', 'maria ', 'uaren ', 'naran ', 'maria ', 'xarana '] after 1.31 minutes.\n",
            "Series  3 names: ['barla ', 'narisa ', 'darisn ', 'narin ', 'qerisa ', 'charisa ', 'rarisa ', 'haris ', 'jarina ', 'rhane '] after 1.28 minutes.\n",
            "Series  4 names: ['xanisa ', 'narin ', 'brine ', 'iarin ', 'derin ', 'tarina ', 'rarin ', 'garisa ', 'nanisia ', 'narin '] after 1.32 minutes.\n",
            "Series  5 names: ['farisa ', 'narisa ', 'qarin ', 'arene ', 'brisisia ', 'annnan ', 'nhanna ', 'marisa ', 'tarin ', 'iarisa '] after 1.31 minutes.\n",
            "Series  6 names: ['harin ', 'uarisa ', 'tarina ', 'varisa ', 'uarin ', 'zarisa ', 'elisis ', 'rarin ', 'jarisa ', 'jarisa '] after 1.30 minutes.\n",
            "Series  7 names: ['anisha ', 'narisha ', 'uanan ', 'warin ', 'qarin ', 'rarisha ', 'darisa ', 'jarisha ', 'zarisha ', 'bren '] after 1.31 minutes.\n",
            "Series  8 names: ['jarin ', 'qanan ', 'narin ', 'brin ', 'rarisha ', 'chanda ', 'jarisha ', 'narisha ', 'harie ', 'anarisa '] after 1.28 minutes.\n",
            "Series  9 names: ['wanisha ', 'ierisha ', 'darisha ', 'farin ', 'xanne ', 'qarisha ', 'rarina ', 'varina ', 'zarisha ', 'elisa '] after 1.27 minutes.\n",
            "Series 10 names: ['varisha ', 'qarisha ', 'nana ', 'marisha ', 'uarin ', 'zarina ', 'zanin ', 'shanne ', 'warin ', 'xarin '] after 1.27 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:3__rand_y:3\n",
            "Series  1 names: ['nanen ', 'wara ', 'alakem ', 'careec ', 'uara ', 'malee ', 'narig ', 'nalca ', 'harale ', 'rara '] after 1.36 minutes.\n",
            "Series  2 names: ['rerah ', 'larla ', 'zanan ', 'uhane ', 'peann ', 'elne ', 'mari ', 'maira ', 'naran ', 'marma '] after 1.24 minutes.\n",
            "Series  3 names: ['xhanea ', 'bavra ', 'nanie ', 'laranee ', 'narra ', 'qielia ', 'charia ', 'varnia ', 'tanne ', 'jerie '] after 1.26 minutes.\n",
            "Series  4 names: ['marra ', 'xania ', 'yarne ', 'vanena ', 'gelis ', 'peyid ', 'watea ', 'zhana ', 'xenan ', 'keiee '] after 1.25 minutes.\n",
            "Series  5 names: ['nalane ', 'naria ', 'nalal ', 'chela ', 'deniel ', 'uanan ', 'baren ', 'yetan ', 'breril ', 'lartia '] after 1.25 minutes.\n",
            "Series  6 names: ['kariar ', 'jaria ', 'sania ', 'hatha ', 'uasisha ', 'arrana ', 'vasin ', 'arisha ', 'ukrha ', 'karie '] after 1.26 minutes.\n",
            "Series  7 names: ['kena ', 'chane ', 'karela ', 'varar ', 'zanana ', 'natiia ', 'uaha ', 'patha ', 'iania ', 'anian '] after 1.24 minutes.\n",
            "Series  8 names: ['naia ', 'ueresha ', 'gasen ', 'dnnia ', 'raren ', 'jaria ', 'qama ', 'shan ', 'zari ', 'paerre '] after 1.25 minutes.\n",
            "Series  9 names: ['waril ', 'xarisha ', 'ninen ', 'annia ', 'frena ', 'cha ', 'brena ', 'haresha ', 'pericn ', 'garria '] after 1.26 minutes.\n",
            "Series 10 names: ['narma ', 'berian ', 'shane ', 'femen ', 'arisha ', 'alera ', 'iande ', 'chania ', 'iandan ', 'paria '] after 1.24 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:6__rand_y:3\n",
            "Series  1 names: ['nara ', 'zena ', 'nara ', 'qara ', 'uana ', 'uara ', 'mara ', 'karie ', 'uara ', 'uara '] after 1.36 minutes.\n",
            "Series  2 names: ['xaran ', 'manan ', 'varee ', 'garia ', 'garee ', 'qaran ', 'raran ', 'berann ', 'maria ', 'naran '] after 1.26 minutes.\n",
            "Series  3 names: ['maria ', 'xeane ', 'xalni ', 'saran ', 'yarian ', 'darie ', 'earia ', 'jara ', 'daran ', 'charia '] after 1.26 minutes.\n",
            "Series  4 names: ['daria ', 'xania ', 'hara ', 'naria ', 'ralera ', 'haria ', 'brana ', 'anania ', 'taria ', 'garie '] after 1.26 minutes.\n",
            "Series  5 names: ['tharia ', 'garia ', 'taria ', 'talesa ', 'chan ', 'naria ', 'xanie ', 'garia ', 'bren ', 'uania '] after 1.27 minutes.\n",
            "Series  6 names: ['briel ', 'naria ', 'jaria ', 'heria ', 'rania ', 'karia ', 'narie ', 'qaria ', 'garia ', 'talea '] after 1.26 minutes.\n",
            "Series  7 names: ['brian ', 'tania ', 'karia ', 'kela ', 'chane ', 'karia ', 'uania ', 'aneria ', 'naria ', 'tania '] after 1.25 minutes.\n",
            "Series  8 names: ['iaria ', 'anania ', 'anana ', 'naria ', 'naria ', 'jaria ', 'deria ', 'vania ', 'anana ', 'xana '] after 1.27 minutes.\n",
            "Series  9 names: ['frisha ', 'eria ', 'caria ', 'wana ', 'xanan ', 'naria ', 'shane ', 'laria ', 'uasha ', 'brisha '] after 1.26 minutes.\n",
            "Series 10 names: ['haria ', 'daria ', 'yania ', 'maria ', 'naria ', 'raria ', 'qaria ', 'faria ', 'qaria ', 'xanan '] after 1.26 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:3__rand_y:5\n",
            "Series  1 names: ['nanel ', 'warala ', 'naria ', 'harane ', 'garana ', 'aaanaria ', 'nanane ', 'larela ', 'tarala ', 'zanna '] after 1.39 minutes.\n",
            "Series  2 names: ['jara ', 'garii ', 'qaran ', 'rerere ', 'uanel ', 'mamna ', 'naran ', 'marla ', 'xheana ', 'basra '] after 1.25 minutes.\n",
            "Series  3 names: ['nariia ', 'darins ', 'narra ', 'qielia ', 'carila ', 'varnia ', 'tennia ', 'jaria ', 'darana ', 'xandra '] after 1.25 minutes.\n",
            "Series  4 names: ['naria ', 'brnan ', 'iarsa ', 'dnnia ', 'taria ', 'garer ', 'trann ', 'amara ', 'rarra ', 'nenia '] after 1.26 minutes.\n",
            "Series  5 names: ['farie ', 'nanian ', 'phala ', 'garian ', 'harel ', 'tania ', 'ieani ', 'parian ', 'farana ', 'wania '] after 1.25 minutes.\n",
            "Series  6 names: ['harin ', 'uarin ', 'iatha ', 'aricin ', 'varisha ', 'xenin ', 'garrice ', 'larria ', 'chane ', 'kariel '] after 1.25 minutes.\n",
            "Series  7 names: ['vareta ', 'uenina ', 'garin ', 'uala ', 'pasha ', 'iarnn ', 'aniele ', 'sanne ', 'xandia ', 'zanis '] after 1.25 minutes.\n",
            "Series  8 names: ['wanda ', 'gara ', 'nrenal ', 'daria ', 'jenin ', 'pakisha ', 'panein ', 'nariey ', 'yanee ', 'haman '] after 1.25 minutes.\n",
            "Series  9 names: ['wanan ', 'latsie ', 'wamishe ', 'iristia ', 'deris ', 'payan ', 'yarien ', 'elina ', 'iamana ', 'yanana '] after 1.26 minutes.\n",
            "Series 10 names: ['warien ', 'zannia ', 'kaman ', 'yarilia ', 'natnin ', 'elanna ', 'tanan ', 'xaner ', 'brinda ', 'larell '] after 1.26 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:6__rand_y:5\n",
            "Series  1 names: ['narei ', 'wara ', 'arana ', 'xania ', 'haria ', 'carana ', 'narie ', 'nara ', 'lara ', 'sania '] after 1.36 minutes.\n",
            "Series  2 names: ['jarii ', 'harie ', 'xaran ', 'narna ', 'naria ', 'brnia ', 'mari ', 'maria ', 'narei ', 'maria '] after 1.26 minutes.\n",
            "Series  3 names: ['xhane ', 'xalnia ', 'naria ', 'laree ', 'narana ', 'earia ', 'jaria ', 'xaria ', 'karie ', 'deria '] after 1.26 minutes.\n",
            "Series  4 names: ['xaria ', 'hanee ', 'varie ', 'naria ', 'chania ', 'narania ', 'varra ', 'daria ', 'qeria ', 'chania '] after 1.25 minutes.\n",
            "Series  5 names: ['narie ', 'haria ', 'naria ', 'chane ', 'darian ', 'uaren ', 'brania ', 'zenel ', 'vania ', 'naria '] after 1.26 minutes.\n",
            "Series  6 names: ['jarian ', 'tanen ', 'zaman ', 'shann ', 'narisha ', 'darie ', 'parian ', 'jarian ', 'uania ', 'haria '] after 1.25 minutes.\n",
            "Series  7 names: ['karisha ', 'harie ', 'hanisha ', 'arisha ', 'frin ', 'chana ', 'aranica ', 'marie ', 'yanee ', 'naria '] after 1.26 minutes.\n",
            "Series  8 names: ['farin ', 'jarisha ', 'zanan ', 'vania ', 'arana ', 'xanan ', 'shana ', 'elia ', 'chania ', 'parin '] after 1.27 minutes.\n",
            "Series  9 names: ['freria ', 'narisha ', 'harisha ', 'caran ', 'larisha ', 'narin ', 'narisha ', 'marin ', 'narin ', 'rarisha '] after 1.26 minutes.\n",
            "Series 10 names: ['vinel ', 'rarin ', 'zanin ', 'narin ', 'xanna ', 'karisha ', 'yanne ', 'narisha ', 'harisha ', 'narisha '] after 1.25 minutes.\n",
            "model finished\n"
          ]
        }
      ],
      "source": [
        "for beam in BEAM:\n",
        "    if beam:\n",
        "        MULTIPLIER = MULTIPLIER_true\n",
        "    else:\n",
        "        MULTIPLIER = MULTIPLIER_false\n",
        "    for rand_y in RAND_Y:\n",
        "        for multiplier in MULTIPLIER:\n",
        "            X, Y, max_len, char_to_int, int_to_char, num_of_chars, chars = get_data(rand_y)\n",
        "            file_name = 'English__beam:' + str(beam) + '__multiplier:' + str(multiplier) + '__rand_y:' + str(rand_y)\n",
        "            print(f'\\nGenerating: {file_name}')\n",
        "\n",
        "            model = get_model()\n",
        "            names_full = []\n",
        "            for i in range(SERIES):\n",
        "                tic = time()\n",
        "                model.fit(X, Y, epochs=EPOCHS, verbose=0, batch_size=64)\n",
        "                names_ = []\n",
        "                for k in range(10):\n",
        "                    n = np.random.randint(1, num_of_chars + 1)\n",
        "                    while n in not_starting:\n",
        "                        n = np.random.randint(1, num_of_chars + 1)\n",
        "                    n = n/num_of_chars\n",
        "\n",
        "                    word = [[]]\n",
        "                    word[0].append(n)\n",
        "                    for j in range(max_len):\n",
        "                        wordX = pad_sequences(word, maxlen=max_len, dtype='float32')\n",
        "                        c = model.predict(wordX.reshape(1, max_len, 1), verbose = 0)\n",
        "                        if beam:\n",
        "                            c = list(c.flatten())\n",
        "                            c = [x ** multiplier for x in c] #premiujemy największe prawdopodobieństwa\n",
        "                            c = [x / sum(c) for x in c]\n",
        "                            c_max = np.random.choice(chars, p = c) # nowa metoda wyboru kolejnej linii; wybierane wg prawdopodobieństwa, a\n",
        "                            c_max = char_to_int[c_max]\n",
        "                        else:\n",
        "                            c_max = int(c.argmax(axis=1)) # stara metoda wybierała zawsze argmax z predict\n",
        "                        word[0].append(c_max/num_of_chars)\n",
        "                        if c_max == 0:\n",
        "                            break\n",
        "                    imie = ''.join([int_to_char[int(char*num_of_chars)] for char in word[0]])\n",
        "                    names_.append(imie)\n",
        "                print(f\"Series {i + 1:>2} names: {names_} after {(time() - tic) / 60:.2f} minutes.\")\n",
        "                names_full.append(names_)\n",
        "\n",
        "            df = pd.DataFrame(names_full)\n",
        "            df.to_csv('/content/drive/MyDrive/Python_data/' + file_name + '.csv')\n",
        "            # print(\"\\nSeries completed:\", i+1, \"\\n\")\n",
        "\n",
        "print('model finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we take a look at the above outcome, we can easily formulate a couple of conclusions:\n",
        "\n",
        "\n",
        "1.   The less randomness, the more repetitive and \"boring\" the outcome\n",
        "2.   The first three results come from a \"greedy search\" procedure and we can see they are are more repetitive than the next six lists with a \"simplified beam\" method\n",
        "1.   Randomness at the word generation level (\"beam\" with lower multiplier value, e.g. 3) adds more flavour than randomness at the training level (randomly modified chars in a train set - rand_y > 0)\n",
        "2.   In this specific task, randomnes does not produce more nonsense, worthless results. If we take a list with \"beam\" True and rand_y positive, we do not find more useless propositions than in series generated without randomness.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8OuX81BFIo7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is obvious that (at least at this level) only a human being can really assess the quality of the outcome. However, we can design some measures of the outcome diversity.\n",
        "Let's analyze each set of names and find out:\n",
        "\n",
        "\n",
        "1.   How many unique names we got\n",
        "2.   How many unique endings (2-grams, 3-grams and 4-grams) we got\n",
        "1.   How many different name lengths we have in a set\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P-PzHPjMKH0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_df_stats(df):\n",
        "    try:\n",
        "        df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
        "    except:\n",
        "        pass\n",
        "    df = df.stack().reset_index()\n",
        "    df.drop(['level_0', 'level_1'], inplace = True, axis = 1)\n",
        "\n",
        "    df.columns = ['name']\n",
        "    unique_len = len(df.name.unique())\n",
        "\n",
        "    df['len'] = df['name'].apply(lambda x: len(x))\n",
        "    df['last2'] = df['name'].apply(lambda x: x[-3:])\n",
        "    df['last3'] = df['name'].apply(lambda x: x[-4:])\n",
        "    df['last4'] = df['name'].apply(lambda x: x[-5:])\n",
        "    len(df['len'].unique())\n",
        "\n",
        "    print(f\"Number of unique names: {unique_len}\")\n",
        "    print(f\"Number of unique last 2 chars: {len(df['last2'].unique())}\")\n",
        "    print(f\"Number of unique last 3 chars: {len(df['last3'].unique())}\")\n",
        "    print(f\"Number of unique last 4 chars: {len(df['last4'].unique())}\")\n",
        "    print(f\"Number of different name lengths: {len(df['len'].unique())}\\n\")"
      ],
      "metadata": {
        "id": "8L_glsngL3QT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAND_Y = [0, 5, 3]\n",
        "MULTIPLIER = [6, 3, 1]\n",
        "BEAM = [False, True]\n",
        "\n",
        "for b in BEAM:\n",
        "    for m in MULTIPLIER:\n",
        "        for r in RAND_Y:\n",
        "            file_name = 'English__beam:' + str(b) + '__multiplier:' + str(m) + '__rand_y:' + str(r)\n",
        "\n",
        "            try:\n",
        "                df = pd.read_csv('/content/drive/MyDrive/Python_data/' + file_name + '.csv')\n",
        "                print(file_name)\n",
        "                print_df_stats(df)\n",
        "\n",
        "            except:\n",
        "                pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgUdU43VL8v2",
        "outputId": "c6d8d686-5acb-4279-806e-8c4f52165213"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English__beam:False__multiplier:1__rand_y:0\n",
            "Number of unique names: 66\n",
            "Number of unique last 2 chars: 9\n",
            "Number of unique last 3 chars: 12\n",
            "Number of unique last 4 chars: 14\n",
            "Number of different name lengths: 3\n",
            "\n",
            "English__beam:False__multiplier:1__rand_y:5\n",
            "Number of unique names: 59\n",
            "Number of unique last 2 chars: 4\n",
            "Number of unique last 3 chars: 8\n",
            "Number of unique last 4 chars: 11\n",
            "Number of different name lengths: 4\n",
            "\n",
            "English__beam:False__multiplier:1__rand_y:3\n",
            "Number of unique names: 59\n",
            "Number of unique last 2 chars: 5\n",
            "Number of unique last 3 chars: 8\n",
            "Number of unique last 4 chars: 20\n",
            "Number of different name lengths: 4\n",
            "\n",
            "English__beam:True__multiplier:6__rand_y:0\n",
            "Number of unique names: 83\n",
            "Number of unique last 2 chars: 16\n",
            "Number of unique last 3 chars: 26\n",
            "Number of unique last 4 chars: 34\n",
            "Number of different name lengths: 5\n",
            "\n",
            "English__beam:True__multiplier:6__rand_y:5\n",
            "Number of unique names: 75\n",
            "Number of unique last 2 chars: 16\n",
            "Number of unique last 3 chars: 28\n",
            "Number of unique last 4 chars: 36\n",
            "Number of different name lengths: 4\n",
            "\n",
            "English__beam:True__multiplier:6__rand_y:3\n",
            "Number of unique names: 72\n",
            "Number of unique last 2 chars: 15\n",
            "Number of unique last 3 chars: 22\n",
            "Number of unique last 4 chars: 34\n",
            "Number of different name lengths: 3\n",
            "\n",
            "English__beam:True__multiplier:3__rand_y:0\n",
            "Number of unique names: 95\n",
            "Number of unique last 2 chars: 22\n",
            "Number of unique last 3 chars: 56\n",
            "Number of unique last 4 chars: 70\n",
            "Number of different name lengths: 5\n",
            "\n",
            "English__beam:True__multiplier:3__rand_y:5\n",
            "Number of unique names: 99\n",
            "Number of unique last 2 chars: 28\n",
            "Number of unique last 3 chars: 57\n",
            "Number of unique last 4 chars: 77\n",
            "Number of different name lengths: 5\n",
            "\n",
            "English__beam:True__multiplier:3__rand_y:3\n",
            "Number of unique names: 98\n",
            "Number of unique last 2 chars: 30\n",
            "Number of unique last 3 chars: 62\n",
            "Number of unique last 4 chars: 80\n",
            "Number of different name lengths: 5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A couple of observations:\n",
        "\n",
        "\n",
        "1.   The first three results with a greedy search (beam: False) have around 60 unique names each of them (out of 100 names generated) and a very limited number of unique 2-gram endings. Adding randomnes at a training data level (data_y = 3 and 5) does not change anything.\n",
        "2.   What really adds value in terms of diversity (without any visible loss in quality) is adding randomness at a text generation level (beam: True)."
      ],
      "metadata": {
        "id": "t6STZoD3K584"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it. I hope you enjoyed it. The model itself is very simple and can be copied from any ML/NLP textbook, but I hope experimenting with randomnes was a fun!"
      ],
      "metadata": {
        "id": "KP2HnpPHLn_Y"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "13-oW1XgwwZRKE2Pd8HKMh0f5Szw3s2j2",
      "authorship_tag": "ABX9TyNE7X5WV+bt4/xxMAw5Gv3q",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}