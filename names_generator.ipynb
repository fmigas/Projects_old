{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13-oW1XgwwZRKE2Pd8HKMh0f5Szw3s2j2",
      "authorship_tag": "ABX9TyM5+W+boupPe42cBeo2CGFs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmigas/Projects/blob/main/names_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to check how adding randomnes to a text-generating model adds quality to the end product.\n",
        "\n",
        "The basis of the model is a simple LSTM network. Its task is to learn English and Polish names and later generate names using different schemes."
      ],
      "metadata": {
        "id": "2KxEDfxFHZgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with the imports."
      ],
      "metadata": {
        "id": "xz3ecmo7J9_6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ggQHdFzGlTe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import os\n",
        "os. environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with loading data - Polish and English names."
      ],
      "metadata": {
        "id": "OU8gYWLSaZKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pnames = pd.read_csv('/content/drive/MyDrive/Python_data/P_names.csv')\n",
        "enames = pd.read_csv('/content/drive/MyDrive/Python_data/E_names.csv')"
      ],
      "metadata": {
        "id": "fUh90YyJSfST"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of Polish names in a database: {len(pnames)}')\n",
        "print(f'Number of English names in a database: {len(enames)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unVB3MAEbRdk",
        "outputId": "868380f1-3d24-441f-d0c6-2d9c8e2b1c3d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Polish names in a database: 1711\n",
            "Number of English names in a database: 18238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have over 10 times as many English names as Polish names. We'll see how it affects the quality of what our model can produce.\n",
        "\n",
        "A fan fact for English readers. In the USA if you have a kid, you can use any word that you wish as his/her name, your fantasty and your warm feelings towards your kid's future in a school is your limit.\n",
        "\n",
        "In Poland (and probably in many other countries) you can only choose from an officially approved list of authorized names. Probably that is a reason why a list of Polish names is over 10 times shorter."
      ],
      "metadata": {
        "id": "rxaSUfZLboZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enames.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Pi96_pP-b63u",
        "outputId": "8dea7f82-6312-48c1-8fe6-b0cc55b0b6bc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          name  len\n",
              "0      michael    7\n",
              "1  christopher   11\n",
              "2      jessica    7\n",
              "3      matthew    7\n",
              "4       ashley    6"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8af9bbb4-a2eb-40a2-99c0-3ed9946dd339\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>michael</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>christopher</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jessica</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>matthew</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ashley</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8af9bbb4-a2eb-40a2-99c0-3ed9946dd339')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8af9bbb4-a2eb-40a2-99c0-3ed9946dd339 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8af9bbb4-a2eb-40a2-99c0-3ed9946dd339');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I preprocessed both files beforehead to save your time here. In both files we have no duplicates, all names lower cased and a 'len' column added with a char count for each name."
      ],
      "metadata": {
        "id": "K9Ni-N3Hb-kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model is naturally a char-level generating model. It's core architecture and data stracture can be easily copied from any deep learning manual.\n",
        "\n",
        "What is interesting here is how we apply randomness at two levels:\n",
        "1. at the learning process by replacing some \"next char\" labels with random chars\n",
        "2. at the generating process by applying what I call a \"simplified Beam\" in place of a greedy search routine\n",
        "\n",
        "Will these two result in better, richer, more interesting and natural names? Let's see!"
      ],
      "metadata": {
        "id": "Nq80U7vVMYoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with English names."
      ],
      "metadata": {
        "id": "d-pm86mLyZmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = enames"
      ],
      "metadata": {
        "id": "7qwtKPAryx73"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a full list of chars used in English names and length of the longest name.\n",
        "As a standard procedure in such models, we also build char_to_int and int_to_char dictionaries."
      ],
      "metadata": {
        "id": "S26PCbwxy1Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = names['len'].max()\n",
        "\n",
        "nlist = list(names['name'])\n",
        "text = ' '.join(nlist)\n",
        "chars = sorted(list(set(text)))\n",
        "print(f'The longest name is {max_len} chars long.')\n",
        "print(f\"Chars list: {chars}\")\n",
        "print(f\"Number of unique chars, including white space: {len(chars)}\")\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hdFEB81y9th",
        "outputId": "db00f1cf-e312-4b7b-ff5c-aff0cb08174f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The longest name is 15 chars long.\n",
            "Chars list: [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Number of unique chars, including white space: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we begin to build our X and Y datasets."
      ],
      "metadata": {
        "id": "URYPBZlH1LBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataX = [] # x data\n",
        "dataY_original = [] # y data (labels) - simply the next char\n",
        "\n",
        "for name in names['name']:\n",
        "    for i in range(len(name)):\n",
        "        if i < len(name) - 1:\n",
        "            dataX.append([char_to_int[char] for char in name[0:i+1]])\n",
        "            dataY_original.append(char_to_int[name[i+1]])\n",
        "        else:\n",
        "            dataX.append([char_to_int[char] for char in name[0:i+1]])\n",
        "            dataY_original.append(0)\n",
        "\n",
        "print('X data')\n",
        "print(dataX[:20])\n",
        "print('Y data')\n",
        "print(dataY_original[:20])\n",
        "originalY = dataY_original[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR2II3Y81THb",
        "outputId": "312065d0-bd91-404e-e4d5-a4f581ae9749"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X data\n",
            "[[13], [13, 9], [13, 9, 3], [13, 9, 3, 8], [13, 9, 3, 8, 1], [13, 9, 3, 8, 1, 5], [13, 9, 3, 8, 1, 5, 12], [3], [3, 8], [3, 8, 18], [3, 8, 18, 9], [3, 8, 18, 9, 19], [3, 8, 18, 9, 19, 20], [3, 8, 18, 9, 19, 20, 15], [3, 8, 18, 9, 19, 20, 15, 16], [3, 8, 18, 9, 19, 20, 15, 16, 8], [3, 8, 18, 9, 19, 20, 15, 16, 8, 5], [3, 8, 18, 9, 19, 20, 15, 16, 8, 5, 18], [10], [10, 5]]\n",
            "Y data\n",
            "[9, 3, 8, 1, 5, 12, 0, 8, 18, 9, 19, 20, 15, 16, 8, 5, 18, 0, 5, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A label (Y data) is simply the next char in a name. That is how all char-based models work at the train phase."
      ],
      "metadata": {
        "id": "uh4jfYCH1nHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we begin adding randomness to the model.\n",
        "At first, we declare a RAND_Y value, which tells, every which char we replace by a random one. If RAND_Y equals zero, there's no randomness and all labels remain unchained."
      ],
      "metadata": {
        "id": "cpqJxSB114Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Y(dataY, RAND_Y = 0):\n",
        "  if RAND_Y > 0:\n",
        "      for i, y in enumerate(dataY):\n",
        "          if y != 0:\n",
        "              if np.random.randint(0, RAND_Y) == 0: # on average, we will replace every RAND_Yth char with a randomly selected char\n",
        "                  dataY[i] = np.random.randint(1, len(chars))\n",
        "  return dataY"
      ],
      "metadata": {
        "id": "ku1RLcxV2Yg6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the first 10 labels look like after various RAND_Y values."
      ],
      "metadata": {
        "id": "crXljCTb6Xd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"No randomness\")\n",
        "dataY = get_Y(dataY_original, RAND_Y = 0)\n",
        "RAND_Y = 0\n",
        "print(f\"Original Y: {originalY}\")\n",
        "print(f\"Modified Y: {dataY[:20]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7tIC6ks99n8",
        "outputId": "792e52df-3c3e-4d44-fb2c-580de6942b93"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No randomness\n",
            "Original Y: [9, 3, 8, 1, 5, 12, 0, 8, 18, 9, 19, 20, 15, 16, 8, 5, 18, 0, 5, 19]\n",
            "Modified Y: [9, 3, 8, 1, 5, 12, 0, 8, 18, 9, 19, 20, 15, 16, 8, 5, 18, 0, 5, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RAND_Y = 7\n",
        "print(f\"Every {RAND_Y}th modified\")\n",
        "dataY = get_Y(dataY_original, RAND_Y)\n",
        "\n",
        "print(f\"Original Y: {originalY}\")\n",
        "print(f\"Modified Y: {dataY[:20]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCYeQsjl9d_Y",
        "outputId": "6c8ae4b9-4688-4f60-e8da-ba162c0bc6c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every 7th modified\n",
            "Original Y: [9, 3, 8, 1, 5, 12, 0, 8, 18, 9, 19, 20, 15, 16, 8, 5, 18, 0, 5, 19]\n",
            "Modified Y: [7, 3, 8, 1, 3, 12, 0, 8, 18, 12, 19, 20, 15, 16, 8, 5, 18, 0, 5, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to prepare data so that they fit an LSTM network.\n",
        "All steps are basic and self-explanatory."
      ],
      "metadata": {
        "id": "MYZosa53DqCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
        "num_of_chars = max(dataY)\n",
        "X = X/num_of_chars\n",
        "Y = np_utils.to_categorical(dataY)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"Y shape: {Y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDchOWqqD05W",
        "outputId": "bf43145b-62c7-4ab8-b5a5-2453cd41a26f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (115316, 15, 1)\n",
            "Y shape: (115316, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build our model. Two layers of Bidirectional LSTM with a serious dropout."
      ],
      "metadata": {
        "id": "5-oq94JIH2uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "  model = Sequential()\n",
        "  model.add(Bidirectional(LSTM(16, return_sequences=True, input_shape=(X.shape[1], 1))))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Bidirectional(LSTM(16)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "mIfr22rqHxCm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've already seen how randomness is added at a training data level. Now let's see how a \"simplified Beam\" is added at a text generating level.\n",
        "\n",
        "A simple choice is a greedy serach. After a predict method produces a vector of \"probabilities\" for each char, we choose the one with the highest value."
      ],
      "metadata": {
        "id": "UyaQK8BWXJz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = model.predict(wordX.reshape(1, max_len, 1), verbose = 0) # a vector\n",
        "c_max = int(c.argmax(axis=1)) # amvalue with a max probability"
      ],
      "metadata": {
        "id": "d0_bducTZnFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An alternative is to choose between a couple of values with the highest score from the probability method. We can randomly choose from a list of all chars using a probability vector as a p parameter in an np.random.choice method.\n",
        "\n",
        "However, if we have a more or less flat vector of probabilities, like let's say 'a' = 0.05, 'b' = 0.07, 'c' = 0.08 etc. we would end with a chaotic noise rather than any interesting output sounding like a real name.\n",
        "\n",
        "To avoid this, we can penalize lower probability values and multiply higher values with a simple trick. We take each vector value and raise to power of MULTIPLIER level. Later, we normalize all values so that all probabilities sum up to 1 and can be used as a p parameter in an np.random.choice method.\n",
        "\n",
        "I usually used MULTIPLIER = 3 value. Let's see how it works.\n",
        "Let's say we have ten chars in our alphabet. We will generate a vector of probabilities for each char and normalize so that they sum up to 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "tvBlFCnCZs5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = np.random.random(10)\n",
        "v = v / sum(v)\n",
        "v = np.sort(v)[::-1]\n",
        "v = np.round(v, 2)\n",
        "print(v)\n",
        "print(np.sum(v[:3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YJZL-JcpvNS",
        "outputId": "3c6a4f8b-a641-43c0-93ec-41e891d1063f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2  0.19 0.17 0.14 0.12 0.09 0.03 0.02 0.02 0.01]\n",
            "0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's assume, that we treat top 2-3 chars as \"good\", which means that choosing out of them we expect to produce a sensible name that \"sounds good\". We should somehow minimze the probability of choosing out of the remaining 7-8 chars, but still keep the order of top 2-3 chars in terms of their probability. On the other hand, we do not want to simply cut off the long tail, rather diminish its probability.\n",
        "\n",
        "Let's see how applying a \"simplified Beam\" with MULTIPLIER = 3 changes our vector."
      ],
      "metadata": {
        "id": "zVLVicmEp2xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MULTIPLIER = 3"
      ],
      "metadata": {
        "id": "5TFWOet3qopC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = [x ** MULTIPLIER for x in v] #premiujemy największe prawdopodobieństwa\n",
        "c = [x / sum(c) for x in c]\n",
        "c = np.round(c, 2)\n",
        "print(c)\n",
        "print(np.sum(c[:3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTZaXOSeqlLl",
        "outputId": "f52ec593-a515-444a-ffae-9e09d6a2a722"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.32 0.27 0.2  0.11 0.07 0.03 0.   0.   0.   0.  ]\n",
            "0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, two top values (0.3 and 0.44) are strengthened and now sum up to 0.93, the other values were made significant.\n",
        "On the other hand, it's still much more random than greedy search."
      ],
      "metadata": {
        "id": "44cJwJ5cZrfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how it looks like with different MULTIPLIER values (from 2 to 6) on a 10 char vector."
      ],
      "metadata": {
        "id": "UnubSvWr2J33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam(vec_len, multiplier):\n",
        "    v = np.random.random(vec_len)\n",
        "    v = v / sum(v)\n",
        "    v = np.sort(v)[::-1]\n",
        "    v = np.round(v, 2)\n",
        "    print('Original vector sorted:')\n",
        "    print(v)\n",
        "    print(f\"Sum of the first three values: {np.sum(v[:3]):.2f}\\n\")\n",
        "\n",
        "    for m in multiplier:\n",
        "        c = [x ** m for x in v] #premiujemy największe prawdopodobieństwa\n",
        "        c = [x / sum(c) for x in c]\n",
        "        c = np.round(c, 2)\n",
        "        print(f\"Vector with multiplier {m}:\")\n",
        "        print(c)\n",
        "        print(f\"Sum of the first three values: {np.sum(c[:3]):.2f}\\n\")\n",
        "\n",
        "\n",
        "beam(vec_len = 10, multiplier = [2, 3, 4, 5, 6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN5wUHZX2S9S",
        "outputId": "ab14f237-547a-46cb-ab01-ef02ff0acab9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vector sorted:\n",
            "[0.16 0.16 0.12 0.11 0.08 0.08 0.08 0.08 0.07 0.05]\n",
            "Sum of the first three values: 0.44\n",
            "\n",
            "Vector with multiplier 2:\n",
            "[0.23 0.23 0.13 0.11 0.06 0.06 0.06 0.06 0.04 0.02]\n",
            "Sum of the first three values: 0.59\n",
            "\n",
            "Vector with multiplier 3:\n",
            "[0.3  0.3  0.13 0.1  0.04 0.04 0.04 0.04 0.02 0.01]\n",
            "Sum of the first three values: 0.73\n",
            "\n",
            "Vector with multiplier 4:\n",
            "[0.35 0.35 0.11 0.08 0.02 0.02 0.02 0.02 0.01 0.  ]\n",
            "Sum of the first three values: 0.81\n",
            "\n",
            "Vector with multiplier 5:\n",
            "[0.39 0.39 0.09 0.06 0.01 0.01 0.01 0.01 0.01 0.  ]\n",
            "Sum of the first three values: 0.87\n",
            "\n",
            "Vector with multiplier 6:\n",
            "[0.42 0.42 0.08 0.04 0.01 0.01 0.01 0.01 0.   0.  ]\n",
            "Sum of the first three values: 0.92\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will try 3 and 6 values in a model."
      ],
      "metadata": {
        "id": "4tyHnUlp2585"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we are now ready with the model.\n",
        "We will generate 10 series of names, 10 in each serie.\n",
        "The model is compiled once at the beginning and it is fit with the data in each series. We will observe how the quality of the generated names gets augmented (or does not?) with each consecitive fit for 10 epochs.\n",
        "\n"
      ],
      "metadata": {
        "id": "XCq6xL-UO5tL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a greedy search and no randomnes in training data!"
      ],
      "metadata": {
        "id": "BZ5qCz03lyyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "names_full = []\n",
        "\n",
        "not_starting = list(np.arange(26, 33)) # only applies to Polish names\n",
        "not_starting = [] # empty for English names\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "print(f\"RandY: {RAND_Y}\")\n",
        "print(f\"Greedy search: {RANDOM_CHARS}\")\n",
        "tic = time()\n",
        "\n",
        "for i in range(SERIES):\n",
        "    tic = time()\n",
        "    model.fit(X, Y, epochs=EPOCHS, verbose=0, batch_size=64)\n",
        "    names_ = []\n",
        "    if i % 1 == 0:\n",
        "        for k in range(10):\n",
        "            n = np.random.randint(1, num_of_chars + 1) #we genearate the first char of the name\n",
        "            while n in not_starting: #used only for Polish names\n",
        "                n = np.random.randint(1, num_of_chars + 1)\n",
        "            n = n/num_of_chars #scaling for an LSTM model\n",
        "\n",
        "            word = [[]]\n",
        "            word[0].append(n)\n",
        "            for j in range(max_len):\n",
        "                wordX = pad_sequences(word, maxlen=max_len, dtype='float32')\n",
        "                c = model.predict(wordX.reshape(1, max_len, 1), verbose = 0)\n",
        "                if RANDOM_CHARS == True:\n",
        "                    c = list(c.flatten())\n",
        "                    c = [x ** MULTIPLIER for x in c]\n",
        "                    c = [x / sum(c) for x in c]\n",
        "                    c_max = np.random.choice(chars, p = c)\n",
        "                    c_max = char_to_int[c_max]\n",
        "                else:\n",
        "                    c_max = int(c.argmax(axis=1))\n",
        "                word[0].append(c_max/num_of_chars)\n",
        "                if c_max == 0:\n",
        "                    break\n",
        "            imie = ''.join([int_to_char[int(char*num_of_chars)] for char in word[0]])\n",
        "\n",
        "            names_.append(imie)\n",
        "\n",
        "        print(f\"Series {i+1:>2} names: {names_} after {(time() - tic)/60:.2f} minutes.\")\n",
        "        names_full.append(names_)\n",
        "\n",
        "        df = pd.DataFrame(names_full)\n",
        "\n",
        "print(f'ALL NAMES GENERATED after {time() - tic:.2f} seconds')\n",
        "df.to_csv('/content/drive/MyDrive/Python_data/English_greedy_norand.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcchQNZPLYDy",
        "outputId": "efaf3480-21d5-4860-e50a-8760a71628b6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandY: 0\n",
            "Greedy search: False\n",
            "Series  1 names: ['wanesha ', 'tanesha ', 'yanesha ', 'narin ', 'garisha ', 'garisha ', 'parin ', 'quan ', 'sharin ', 'tanesha ']\n",
            "Series  2 names: ['kamesha ', 'farrin ', 'parin ', 'marin ', 'zanesha ', 'delisha ', 'tanesha ', 'harin ', 'farrin ', 'shanna ']\n",
            "Series  3 names: ['zachell ', 'harrin ', 'larisha ', 'yachell ', 'larisha ', 'ranina ', 'antell ', 'tarin ', 'nlena ', 'harrin ']\n",
            "Series  4 names: ['wanesha ', 'istan ', 'marisha ', 'urenta ', 'nrenta ', 'urenta ', 'qunell ', 'yanesha ', 'ranesha ', 'elista ']\n",
            "Series  5 names: ['elista ', 'jamisha ', 'latan ', 'farris ', 'ista ', 'elista ', 'urenta ', 'nristin ', 'brittin ', 'urenta ']\n",
            "Series  6 names: ['ranesha ', 'urena ', 'jamina ', 'harrie ', 'yanesha ', 'walina ', 'lataria ', 'ishanna ', 'antnn ', 'charlina ']\n",
            "Series  7 names: ['isanna ', 'urentin ', 'quanna ', 'gerrica ', 'elisha ', 'gerrica ', 'darricia ', 'latara ', 'urentin ', 'walena ']\n",
            "Series  8 names: ['yanesha ', 'willian ', 'brittin ', 'harrick ', 'gerrica ', 'tarina ', 'villian ', 'charlina ', 'villian ', 'marisha ']\n",
            "Series  9 names: ['vinisha ', 'yanesha ', 'marisha ', 'yanesha ', 'harrin ', 'tanesha ', 'urenta ', 'xanesha ', 'tanesha ', 'gerrica ']\n",
            "Series 10 names: ['antnn ', 'elisha ', 'randi ', 'antnn ', 'walina ', 'shanell ', 'natasha ', 'walina ', 'isabell ', 'quanna ']\n",
            "ALL NAMES GENERATED after 3463.29 sekund\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import random as python_random\n",
        "from time import time\n",
        "\n",
        "def reset_seeds():\n",
        "   np.random.seed(123)\n",
        "   python_random.seed(123)\n",
        "   tf.random.set_seed(1234)\n",
        "\n",
        "def get_data(rand_y):\n",
        "    names = pd.read_csv('/content/drive/MyDrive/Python_data/E_names.csv')\n",
        "    max_len = names['len'].max()\n",
        "\n",
        "    nlist = list(names['name'])\n",
        "    text = ' '.join(nlist)\n",
        "    chars = sorted(list(set(text)))\n",
        "\n",
        "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "    dataX = [] # x data\n",
        "    dataY = [] # y data (labels) - simply the next char\n",
        "\n",
        "    for name in names['name']:\n",
        "        for i in range(len(name)):\n",
        "            if i < len(name) - 1:\n",
        "                dataX.append([char_to_int[char] for char in name[0:i+1]])\n",
        "                dataY.append(char_to_int[name[i+1]])\n",
        "            else:\n",
        "                dataX.append([char_to_int[char] for char in name[0:i+1]])\n",
        "                dataY.append(0)\n",
        "\n",
        "    if rand_y > 0:\n",
        "        for i, y in enumerate(dataY):\n",
        "            if y != 0:\n",
        "                if np.random.randint(0, rand_y) == 0: # on average, we will replace every RAND_Yth char with a randomly selected char\n",
        "                    # print('losowo')\n",
        "                    dataY[i] = np.random.randint(1, len(chars))\n",
        "\n",
        "\n",
        "    X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
        "    num_of_chars = max(dataY)\n",
        "    X = X/num_of_chars\n",
        "    Y = np_utils.to_categorical(dataY)\n",
        "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "    return X, Y, max_len, char_to_int, int_to_char, num_of_chars, chars\n"
      ],
      "metadata": {
        "id": "tRsg99QQk9bU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For speed, we will use a slightly smaller network with 16 neurons in each of two LSTM layers.\n",
        "For repeatability, we will reset seed each time at the model restart."
      ],
      "metadata": {
        "id": "ZV8kbbzjn0W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    reset_seeds()\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(16, return_sequences=True, input_shape=(X.shape[1], 1))))\n",
        "    # model.add(Bidirectional(LSTM(16, input_shape=(X.shape[1], 1))))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Bidirectional(LSTM(16)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "vxYxcohzljYw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_starting = list(np.arange(26, 33)) # only applies to Polish names\n",
        "not_starting = [] # only applies to Polish names\n",
        "\n",
        "RAND_Y = [0, 3, 5]\n",
        "MULTIPLIER_true = [3, 6]\n",
        "MULTIPLIER_false = [1]\n",
        "BEAM = [False, True]\n",
        "EPOCHS = 10\n",
        "SERIES = 10"
      ],
      "metadata": {
        "id": "YTClKuuntyka"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for beam in BEAM:\n",
        "    if beam:\n",
        "        MULTIPLIER = MULTIPLIER_true\n",
        "    else:\n",
        "        MULTIPLIER = MULTIPLIER_false\n",
        "    for rand_y in RAND_Y:\n",
        "        for multiplier in MULTIPLIER:\n",
        "            X, Y, max_len, char_to_int, int_to_char, num_of_chars, chars = get_data(rand_y)\n",
        "            file_name = 'English__beam:' + str(beam) + '__multiplier:' + str(multiplier) + '__rand_y:' + str(rand_y)\n",
        "            print(f'\\nGenerating: {file_name}')\n",
        "\n",
        "            model = get_model()\n",
        "            names_full = []\n",
        "            for i in range(SERIES):\n",
        "                tic = time()\n",
        "                model.fit(X, Y, epochs=EPOCHS, verbose=0, batch_size=64)\n",
        "                names_ = []\n",
        "                for k in range(10):\n",
        "                    n = np.random.randint(1, num_of_chars + 1)\n",
        "                    while n in not_starting:\n",
        "                        n = np.random.randint(1, num_of_chars + 1)\n",
        "                    n = n/num_of_chars\n",
        "\n",
        "                    word = [[]]\n",
        "                    word[0].append(n)\n",
        "                    for j in range(max_len):\n",
        "                        wordX = pad_sequences(word, maxlen=max_len, dtype='float32')\n",
        "                        c = model.predict(wordX.reshape(1, max_len, 1), verbose = 0)\n",
        "                        if beam:\n",
        "                            c = list(c.flatten())\n",
        "                            c = [x ** multiplier for x in c] #premiujemy największe prawdopodobieństwa\n",
        "                            c = [x / sum(c) for x in c]\n",
        "                            c_max = np.random.choice(chars, p = c) # nowa metoda wyboru kolejnej linii; wybierane wg prawdopodobieństwa, a\n",
        "                            c_max = char_to_int[c_max]\n",
        "                        else:\n",
        "                            c_max = int(c.argmax(axis=1)) # stara metoda wybierała zawsze argmax z predict\n",
        "                        word[0].append(c_max/num_of_chars)\n",
        "                        if c_max == 0:\n",
        "                            break\n",
        "                    imie = ''.join([int_to_char[int(char*num_of_chars)] for char in word[0]])\n",
        "                    names_.append(imie)\n",
        "                print(f\"Series {i + 1:>2} names: {names_} after {(time() - tic) / 60:.2f} minutes.\")\n",
        "                names_full.append(names_)\n",
        "\n",
        "            df = pd.DataFrame(names_full)\n",
        "            df.to_csv(file_name+'.csv')\n",
        "            # print(\"\\nSeries completed:\", i+1, \"\\n\")\n",
        "\n",
        "print('model finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFzUazRdt55Z",
        "outputId": "6ef2a070-36fc-41a8-c54e-b6089fbead85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating: English__beam:False__multiplier:1__rand_y:0\n",
            "Series  1 names: ['nara ', 'caran ', 'caran ', 'gara ', 'raran ', 'taran ', 'kara ', 'zaran ', 'waran ', 'brria '] after 2.48 minutes.\n",
            "Series  2 names: ['annnel ', 'rarin ', 'parin ', 'jarin ', 'annnel ', 'narin ', 'annnel ', 'parin ', 'zarin ', 'tarin '] after 2.33 minutes.\n",
            "Series  3 names: ['narin ', 'elina ', 'andrin ', 'qarin ', 'elina ', 'rarin ', 'xarina ', 'darin ', 'chane ', 'uarina '] after 2.28 minutes.\n",
            "Series  4 names: ['harin ', 'channa ', 'uarin ', 'puanna ', 'yarin ', 'qarin ', 'harin ', 'jarin ', 'darin ', 'uarin '] after 2.25 minutes.\n",
            "Series  5 names: ['garin ', 'branna ', 'varin ', 'warin ', 'channa ', 'branna ', 'marin ', 'iarin ', 'darin ', 'karin '] after 2.27 minutes.\n",
            "Series  6 names: ['farin ', 'annell ', 'larin ', 'channa ', 'karin ', 'warin ', 'narin ', 'shana ', 'elisha ', 'puanna '] after 2.27 minutes.\n",
            "Series  7 names: ['larin ', 'marin ', 'uarisha ', 'garisha ', 'narin ', 'tarisha ', 'qurin ', 'garisha ', 'nrista ', 'uarisha '] after 2.26 minutes.\n",
            "Series  8 names: ['harisha ', 'larisha ', 'warin ', 'harisha ', 'brista ', 'larisha ', 'farisha ', 'xarin ', 'zarin ', 'shana '] after 2.28 minutes.\n",
            "Series  9 names: ['urista ', 'yarisha ', 'rarisha ', 'marisha ', 'shana ', 'rarisha ', 'brista ', 'tarisha ', 'marisha ', 'varisha '] after 2.26 minutes.\n",
            "Series 10 names: ['jarin ', 'qurin ', 'yanin ', 'rarisha ', 'wanin ', 'darin ', 'darin ', 'larin ', 'vanin ', 'zanin '] after 2.29 minutes.\n",
            "\n",
            "Generating: English__beam:False__multiplier:1__rand_y:3\n",
            "Series  1 names: ['nara ', 'caran ', 'caran ', 'gara ', 'rara ', 'taran ', 'kara ', 'zanan ', 'wanan ', 'brana '] after 2.39 minutes.\n",
            "Series  2 names: ['arana ', 'raria ', 'paria ', 'jaria ', 'arana ', 'naria ', 'arana ', 'paria ', 'zania ', 'taria '] after 2.31 minutes.\n",
            "Series  3 names: ['naria ', 'eria ', 'anaria ', 'qaria ', 'eria ', 'raria ', 'xania ', 'daria ', 'charia ', 'uania '] after 2.31 minutes.\n",
            "Series  4 names: ['harin ', 'chana ', 'uania ', 'parin ', 'yania ', 'qaria ', 'harin ', 'jarin ', 'darin ', 'uania '] after 2.30 minutes.\n",
            "Series  5 names: ['garin ', 'brin ', 'vanin ', 'wanin ', 'chan ', 'brin ', 'marin ', 'iarin ', 'darin ', 'karin '] after 2.31 minutes.\n",
            "Series  6 names: ['farin ', 'anin ', 'larin ', 'chana ', 'karin ', 'wana ', 'narin ', 'shana ', 'elen ', 'parin '] after 2.31 minutes.\n",
            "Series  7 names: ['larin ', 'marin ', 'uanna ', 'garin ', 'narin ', 'tanna ', 'qarin ', 'garin ', 'nrin ', 'uanna '] after 2.32 minutes.\n",
            "Series  8 names: ['harin ', 'larin ', 'wanna ', 'harin ', 'brin ', 'larin ', 'farin ', 'xanna ', 'zanna ', 'shana '] after 2.31 minutes.\n",
            "Series  9 names: ['uanna ', 'yanna ', 'rarin ', 'marin ', 'shana ', 'rarin ', 'brin ', 'tanna ', 'marin ', 'vanna '] after 2.32 minutes.\n",
            "Series 10 names: ['jarin ', 'qunn ', 'yanna ', 'ranna ', 'wanna ', 'darin ', 'darin ', 'larin ', 'vanna ', 'zanna '] after 2.31 minutes.\n",
            "\n",
            "Generating: English__beam:False__multiplier:1__rand_y:5\n",
            "Series  1 names: ['nara ', 'carie ', 'carie ', 'garia ', 'rara ', 'tara ', 'karia ', 'zanan ', 'wanan ', 'brana '] after 2.42 minutes.\n",
            "Series  2 names: ['anaria ', 'raria ', 'paria ', 'jaria ', 'anaria ', 'naria ', 'anaria ', 'paria ', 'zania ', 'tania '] after 2.32 minutes.\n",
            "Series  3 names: ['naria ', 'elana ', 'anana ', 'qaria ', 'elana ', 'rania ', 'xania ', 'daria ', 'chana ', 'uania '] after 2.31 minutes.\n",
            "Series  4 names: ['harisha ', 'charin ', 'uaria ', 'parisha ', 'yanel ', 'qarisha ', 'harisha ', 'jarisha ', 'darisha ', 'uaria '] after 2.34 minutes.\n",
            "Series  5 names: ['garisha ', 'briste ', 'varisha ', 'warisha ', 'charin ', 'briste ', 'marisha ', 'iriste ', 'darisha ', 'karisha '] after 2.33 minutes.\n",
            "Series  6 names: ['farisha ', 'anisha ', 'larisha ', 'charis ', 'karisha ', 'wanan ', 'narisha ', 'shana ', 'elerin ', 'parisha '] after 2.34 minutes.\n",
            "Series  7 names: ['larisha ', 'marisha ', 'uana ', 'garisha ', 'narisha ', 'tana ', 'quan ', 'garisha ', 'nrisha ', 'uana '] after 2.33 minutes.\n",
            "Series  8 names: ['harisha ', 'larisha ', 'wana ', 'harisha ', 'briste ', 'larisha ', 'farisha ', 'xana ', 'zana ', 'shana '] after 2.34 minutes.\n",
            "Series  9 names: ['uarisha ', 'yarisha ', 'rarisha ', 'marisha ', 'shana ', 'rarisha ', 'briste ', 'tarisha ', 'marisha ', 'varisha '] after 2.33 minutes.\n",
            "Series 10 names: ['jarisha ', 'quana ', 'yana ', 'rarisha ', 'wana ', 'darisha ', 'darisha ', 'larisha ', 'vana ', 'zana '] after 2.33 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:3__rand_y:0\n",
            "Series  1 names: ['nanen ', 'warana ', 'naria ', 'hara ', 'deree ', 'brria ', 'waneria ', 'qaris ', 'faria ', 'taneia '] after 2.42 minutes.\n",
            "Series  2 names: ['zanna ', 'jara ', 'garis ', 'qaran ', 'reresia ', 'mari ', 'malina ', 'naran ', 'marin ', 'xhanela '] after 2.33 minutes.\n",
            "Series  3 names: ['frrel ', 'xarin ', 'delist ', 'narnn ', 'qilerh ', 'charler ', 'rherela ', 'denel ', 'xarile ', 'mandea '] after 2.31 minutes.\n",
            "Series  4 names: ['qarisha ', 'eline ', 'darisha ', 'dnrice ', 'felele ', 'rarisa ', 'chele ', 'puand ', 'garisha ', 'naman '] after 2.33 minutes.\n",
            "Series  5 names: ['chande ', 'jarisha ', 'asena ', 'brerine ', 'nhane ', 'ierisha ', 'anter ', 'shira ', 'nran ', 'darin '] after 2.33 minutes.\n",
            "Series  6 names: ['rarin ', 'parara ', 'xasha ', 'anth ', 'thana ', 'prenis ', 'puena ', 'larrie ', 'chania ', 'nandie '] after 2.31 minutes.\n",
            "Series  7 names: ['isara ', 'uanina ', 'garin ', 'uala ', 'patha ', 'ilanie ', 'jamesha ', 'farla ', 'jarrie ', 'rarna '] after 2.33 minutes.\n",
            "Series  8 names: ['wanan ', 'elalena ', 'puiane ', 'nrisha ', 'walane ', 'marin ', 'xarisha ', 'ninen ', 'annia ', 'frann '] after 2.33 minutes.\n",
            "Series  9 names: ['cha ', 'brenie ', 'helin ', 'derishe ', 'garrin ', 'narma ', 'brishe ', 'shani ', 'frenin ', 'antan '] after 2.33 minutes.\n",
            "Series 10 names: ['alera ', 'irine ', 'channa ', 'inenan ', 'perelia ', 'zaria ', 'marrin ', 'karish ', 'brista ', 'larisha '] after 2.31 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:6__rand_y:0\n",
            "Series  1 names: ['naren ', 'waran ', 'zanen ', 'caran ', 'dara ', 'caran ', 'karia ', 'uaran ', 'laree ', 'yaria '] after 2.44 minutes.\n",
            "Series  2 names: ['yarin ', 'chann ', 'genela ', 'uanna ', 'paren ', 'erin ', 'maria ', 'uaran ', 'naran ', 'marin '] after 2.34 minutes.\n",
            "Series  3 names: ['xaran ', 'xalisa ', 'nhanel ', 'delisia ', 'warana ', 'farin ', 'darin ', 'zarin ', 'denan ', 'felel '] after 2.35 minutes.\n",
            "Series  4 names: ['eleria ', 'xarin ', 'puane ', 'elene ', 'darisia ', 'derin ', 'tarina ', 'rarin ', 'garin ', 'fanel '] after 2.34 minutes.\n",
            "Series  5 names: ['tarin ', 'tameta ', 'chanda ', 'zaresha ', 'uaren ', 'blanna ', 'zenela ', 'ianan ', 'parina ', 'farisha '] after 2.32 minutes.\n",
            "Series  6 names: ['channa ', 'karin ', 'nranna ', 'xarisha ', 'yaresha ', 'aran ', 'uanen ', 'harin ', 'karisha ', 'harin '] after 2.34 minutes.\n",
            "Series  7 names: ['harin ', 'zarin ', 'wanera ', 'larisha ', 'parin ', 'uarin ', 'garin ', 'nrine ', 'frisha ', 'jarin '] after 2.35 minutes.\n",
            "Series  8 names: ['derisha ', 'bran ', 'jarisha ', 'darin ', 'jarisha ', 'narin ', 'puisha ', 'narine ', 'yarin ', 'harisha '] after 2.37 minutes.\n",
            "Series  9 names: ['chana ', 'larisha ', 'narina ', 'nranna ', 'puistan ', 'xarisha ', 'larin ', 'jarisha ', 'antha ', 'alan '] after 2.36 minutes.\n",
            "Series 10 names: ['zanen ', 'alene ', 'brina ', 'uarisha ', 'tanina ', 'marin ', 'karisha ', 'zanisha ', 'shanel ', 'shanne '] after 2.35 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:3__rand_y:3\n",
            "Series  1 names: ['nanei ', 'wanal ', 'zaien ', 'carea ', 'dera ', 'carai ', 'kalnia ', 'nalda ', 'hara ', 'yarie '] after 2.45 minutes.\n",
            "Series  2 names: ['yanla ', 'cheri ', 'gelea ', 'kara ', 'iarla ', 'darrl ', 'uanel ', 'maina ', 'narcl ', 'menla '] after 2.35 minutes.\n",
            "Series  3 names: ['xera ', 'yan ', 'chanca ', 'denena ', 'zava ', 'wana ', 'yakan ', 'charien ', 'rasha ', 'tania '] after 2.35 minutes.\n",
            "Series  4 names: ['jerii ', 'marra ', 'xania ', 'yasha ', 'vaman ', 'heeria ', 'karia ', 'taran ', 'naran ', 'xanal '] after 2.35 minutes.\n",
            "Series  5 names: ['kelel ', 'namani ', 'nathe ', 'nama ', 'annia ', 'annen ', 'narian ', 'yauie ', 'qana ', 'ranee '] after 2.34 minutes.\n",
            "Series  6 names: ['ieann ', 'patresha ', 'cashe ', 'cena ', 'hasha ', 'uanel ', 'iatha ', 'anian ', 'varin ', 'ancha '] after 2.36 minutes.\n",
            "Series  7 names: ['yasha ', 'uanele ', 'kena ', 'chania ', 'narin ', 'veran ', 'zana ', 'tendria ', 'chal ', 'iara '] after 2.35 minutes.\n",
            "Series  8 names: ['maria ', 'yanee ', 'nari ', 'darria ', 'jarria ', 'rerra ', 'wana ', 'gara ', 'nlendi ', 'darra '] after 2.35 minutes.\n",
            "Series  9 names: ['jenen ', 'padisha ', 'paria ', 'fanna ', 'xesha ', 'narki ', 'brile ', 'nure ', 'brele ', 'haran '] after 2.35 minutes.\n",
            "Series 10 names: ['xannia ', 'yanell ', 'uanen ', 'shanic ', 'charia ', 'jaren ', 'deshe ', 'vina ', 'narrel ', 'nrina '] after 2.33 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:6__rand_y:3\n",
            "Series  1 names: ['naree ', 'waran ', 'zanan ', 'caran ', 'daran ', 'brera ', 'wanane ', 'taree ', 'brree ', 'sara '] after 2.46 minutes.\n",
            "Series  2 names: ['rara ', 'chari ', 'gele ', 'narre ', 'naria ', 'briia ', 'mare ', 'maria ', 'naran ', 'maria '] after 2.35 minutes.\n",
            "Series  3 names: ['xara ', 'yaran ', 'zaria ', 'larie ', 'xenia ', 'wara ', 'yama ', 'paria ', 'xatha ', 'karie '] after 2.36 minutes.\n",
            "Series  4 names: ['derie ', 'xanie ', 'harin ', 'vaneria ', 'vanen ', 'harin ', 'vamira ', 'tara ', 'raria ', 'varina '] after 2.36 minutes.\n",
            "Series  5 names: ['harin ', 'narin ', 'harin ', 'narin ', 'chan ', 'aline ', 'naria ', 'yana ', 'farin ', 'fenel '] after 2.36 minutes.\n",
            "Series  6 names: ['inan ', 'harin ', 'marisha ', 'jarin ', 'sharie ', 'uanel ', 'ilin ', 'tarie ', 'yana ', 'tanen '] after 2.34 minutes.\n",
            "Series  7 names: ['shana ', 'garin ', 'elisha ', 'rarin ', 'jarin ', 'jarin ', 'vanda ', 'elisha ', 'shanel ', 'ilen '] after 2.36 minutes.\n",
            "Series  8 names: ['uanda ', 'garin ', 'nrin ', 'sanne ', 'xanen ', 'henisha ', 'wana ', 'gara ', 'nrin ', 'quann '] after 2.35 minutes.\n",
            "Series  9 names: ['nrisha ', 'wala ', 'garisha ', 'xanda ', 'nrisha ', 'nneri ', 'brin ', 'uarin ', 'bren ', 'zanna '] after 2.35 minutes.\n",
            "Series 10 names: ['zerisha ', 'yanina ', 'uande ', 'shanin ', 'charin ', 'jarin ', 'derisha ', 'alera ', 'ilene ', 'chann '] after 2.40 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:3__rand_y:5\n",
            "Series  1 names: ['nanei ', 'warana ', 'naria ', 'hara ', 'dara ', 'carana ', 'natie ', 'nama ', 'leana ', 'yarhe '] after 2.46 minutes.\n",
            "Series  2 names: ['yarna ', 'chari ', 'gelea ', 'karai ', 'warani ', 'erne ', 'mari ', 'makla ', 'nares ', 'maria '] after 2.35 minutes.\n",
            "Series  3 names: ['xhalel ', 'batra ', 'narie ', 'larana ', 'daill ', 'elnia ', 'jarela ', 'chanie ', 'alara ', 'sharia '] after 2.35 minutes.\n",
            "Series  4 names: ['shann ', 'iiane ', 'qarrie ', 'vanana ', 'geris ', 'payni ', 'wava ', 'daria ', 'qaria ', 'chele '] after 2.36 minutes.\n",
            "Series  5 names: ['pela ', 'tanele ', 'darsa ', 'chan ', 'narie ', 'xanni ', 'genell ', 'haric ', 'tarie ', 'irani '] after 2.35 minutes.\n",
            "Series  6 names: ['parian ', 'faresha ', 'chanda ', 'karia ', 'narisha ', 'denin ', 'parian ', 'jarien ', 'ualen ', 'handa '] after 2.37 minutes.\n",
            "Series  7 names: ['karrin ', 'karin ', 'eleela ', 'zaria ', 'wameria ', 'chana ', 'alanda ', 'uana ', 'nata ', 'rasen '] after 2.36 minutes.\n",
            "Series  8 names: ['xarina ', 'jarelia ', 'zanisa ', 'wana ', 'gara ', 'nenell ', 'darla ', 'jnnen ', 'painia ', 'weril '] after 2.36 minutes.\n",
            "Series  9 names: ['xanan ', 'nistel ', 'narli ', 'brina ', 'ntre ', 'brelia ', 'henin ', 'derle ', 'yanell ', 'uanana '] after 2.37 minutes.\n",
            "Series 10 names: ['marle ', 'charie ', 'jerin ', 'deshe ', 'vina ', 'narrin ', 'nisha ', 'brina ', 'uatha ', 'nathan '] after 2.34 minutes.\n",
            "\n",
            "Generating: English__beam:True__multiplier:6__rand_y:5\n"
          ]
        }
      ]
    }
  ]
}